{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6ba2d28",
   "metadata": {},
   "source": [
    "# Libraries used\n",
    "PV --> Running 3.10.10\n",
    "Running Kernel3.9.13 base anaconda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47ff5b1",
   "metadata": {},
   "source": [
    "#perform al pip installs in one go comment out if already installed\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install cufflinks\n",
    "!pip install squarify\n",
    "!pip install yellowbrick\n",
    "!pip install lazypredict\n",
    "!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869b5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "#import squarify #treemap\n",
    "import os\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "#to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f230c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.style.palettes import PALETTES, SEQUENCES, color_palette\n",
    "\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19146b48",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4daab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for EDA. Using the display() function to have  well-formatted tables. We are mainly using pandas to explore the datasets\n",
    "\n",
    "def dataset_description(df_target):\n",
    "\n",
    "    print('This is the Dataset shape: %s\\n' % (df_target.shape, ))\n",
    "    print('Dataset columns: %s\\n' % df_target.columns)\n",
    "\n",
    "    print('\\nColumns description:\\n')\n",
    "    display(df_target.info())\n",
    "    display(df_target.describe())  # describe the dataset\n",
    "\n",
    "    print('\\nNull values:\\n')\n",
    "    display(df_target.isnull().sum())  # Identify null values\n",
    "\n",
    "#function performing a quick check on df_inspection to have best of pandas functions separated by a line\n",
    "def quick_check(dataframe):\n",
    "    print('First 5 rows %s\\n')\n",
    "    print(dataframe.head(2))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe shape %s\\n')\n",
    "    print(dataframe.shape)\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe describe categorical %s\\n')\n",
    "    print(dataframe.describe(include=['O']))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe null values %s\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe value counts %s\\n')\n",
    "    print(dataframe.value_counts())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#stats function\n",
    "def stats(dataframe):\n",
    "    print('Dataframe correlation %s\\n')\n",
    "    print(dataframe.corr())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe covariance %s\\n')\n",
    "    print(dataframe.cov())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe skew %s\\n')\n",
    "    print(dataframe.skew())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe kurtosis %s\\n')\n",
    "    print(dataframe.kurt())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#create a function to normalize characters from a dataset's column in Spanish\n",
    "def normalize_characters(df, column):\n",
    "    df[column] = df[column].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace('á', 'a')\n",
    "    df[column] = df[column].str.replace('é', 'e')\n",
    "    df[column] = df[column].str.replace('í', 'i')\n",
    "    df[column] = df[column].str.replace('ó', 'o')\n",
    "    df[column] = df[column].str.replace('ú', 'u')\n",
    "    df[column] = df[column].str.replace('ñ', 'n')\n",
    "    df[column] = df[column].str.replace('ü', 'u')\n",
    "    df[column] = df[column].str.replace('ç', 'c')\n",
    "    df[column] = df[column].str.replace('(', '')\n",
    "    df[column] = df[column].str.replace(')', '')\n",
    "    df[column] = df[column].str.replace('\\'', '')\n",
    "    df[column] = df[column].str.replace('´', '')\n",
    "    df[column] = df[column].str.replace('`', '')\n",
    "    df[column] = df[column].str.replace('’', '')\n",
    "    return df.head(2)\n",
    "\n",
    "#create function to change detypes in64 to int32 in a df\n",
    "def change_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "def outlier_function(df, col_name):\n",
    "    \"\"\" this function detects first and third quartile and interquartile range for a given column of a dataframe\n",
    "    then calculates upper and lower limits to determine outliers conservatively\n",
    "    returns the number of lower and uper limit and number of outliers respectively\"\"\"\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                        \n",
    "    upper_limit = third_quartile+(3*IQR)\n",
    "    lower_limit = first_quartile-(3*IQR)\n",
    "    outlier_count = 0\n",
    "                    \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "        else:\n",
    "            pass\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac38c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all print outputs when using a function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a7d05",
   "metadata": {},
   "source": [
    "# 2. Data Collection and Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b331027",
   "metadata": {},
   "source": [
    "## Network file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931606e2",
   "metadata": {},
   "source": [
    "The network.csv file contains a static picture of the gas pipeline network. Every row corresponds to a pipe and has a unique PipeId identifier. The table has 1.446.529 pipes. \n",
    "\n",
    "The columns describe relevant features of each pipe. The complete list is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2859c",
   "metadata": {},
   "source": [
    "PipeId - unique identifier for the pipe \n",
    "\n",
    "Province - Spanish province where the pipe is located \n",
    "\n",
    "Town - Town or city where the pipe is located \n",
    "\n",
    "YearBuilt - Year in which the pipe was built and installed \n",
    "\n",
    "Material - Material in which the pipe is built \n",
    "\n",
    "GasType - Type of gas that runs through the pipe \n",
    "\n",
    "Diameter - diameter of the pipe \n",
    "\n",
    "Length - Length of the pipe \n",
    "\n",
    "Pressure - Pressure of the gas that runs through the pipe (bar) \n",
    "\n",
    "NumConnections - Number of connections (external). \n",
    "\n",
    "NumConnectionsUnder - Number of connections (internal and buried) \n",
    "\n",
    "BoolBridle  - Whether the pipe is bridled (True) or welded (False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d409b09",
   "metadata": {},
   "source": [
    "## Inspection file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc407aa",
   "metadata": {},
   "source": [
    "MaintenanceId - unique identifier for the inspection operation \n",
    "\n",
    "InspectionYear - year in which the inspection took place \n",
    "\n",
    "InspectionDate - date in which the inspection took place \n",
    "\n",
    "MonthsLastRev - number of months elapsed since the last previous inspection. \n",
    "\n",
    "Severity - Severity of the damage found (1: most severe, 3: least severe) \n",
    "\n",
    "Incidence - Boolean whether an incident was found on the revision (1) or not (0). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7ed6752",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57078a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Pedro\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\")\n",
    "\n",
    "# Loading inspection data\n",
    "df_inspection = pd.read_csv('inspections.csv')\n",
    "\n",
    "# Loading network data\n",
    "df_network = pd.read_csv('network.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Pedro\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\\Datathon-Rules-and-Documentation\") \n",
    "\n",
    "#Loading sample_submission\n",
    "df_sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53839ee",
   "metadata": {},
   "source": [
    "#point to the folder where the data is stored For Juan\n",
    "os.chdir(r\"C:\\Users\\JuanHorrillo\\OneDrive - IE Students\\Documents\\Masters\\Sustainability\\Notebook\")\n",
    "\n",
    "# Loading inspection data\n",
    "df_inspection = pd.read_csv('inspections.csv')\n",
    "\n",
    "# Loading network data\n",
    "df_network = pd.read_csv('network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_check(df_inspection)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "942a7459",
   "metadata": {},
   "source": [
    "The function shows we have 6345344 rows for the inspections, among each 4179 appear to be unique and the most repeated one is ZRV-00002121 on 2014-05-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for MaintenanceId ZRV-00002121 in df_inspection\n",
    "df_inspection[df_inspection['MaintenanceId'] == 'ZRV-00002121']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inspection[df_inspection['MaintenanceId'] == 'ZRV-00002121'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the distribution of MaintenanceId accross the years for ZRV-00002121 in df_inspection\n",
    "df_inspection[df_inspection['MaintenanceId'] == 'ZRV-00002121'].groupby('InspectionYear')['MaintenanceId'].count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ea9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the distribution of MaintenanceId accross the years\n",
    "df_inspection.groupby('InspectionYear')['MaintenanceId'].count().plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7fc912",
   "metadata": {},
   "source": [
    "Seems like not so many inspections were carried in 2010, we shall decide on whether to keep this dimension or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(df_inspection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abd572cb",
   "metadata": {},
   "source": [
    " A positive kurtosis in a dataset implies that the data is more heavily concentrated around the mean than a normal distribution. This means that there are more outliers in the dataset and that the tails of the distribution are longer and fatter than a normal distribution. Months last revision is to be examined on this basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram with many bins of MonthsLastRev in  df_inspection\n",
    "df_inspection['MonthsLastRev'].hist(bins=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a3aca2",
   "metadata": {},
   "source": [
    "The shape of the histogram reveals the most frequent inspections take place every 2 years or 24 months with some outliers hence affecting the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f603d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot violin plot of PipeId with Severity=1 in df_inspection\n",
    "sns.violinplot(x='PipeId', y='MonthsLastRev', data=df_inspection[df_inspection['Severity'] == 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09db2f99",
   "metadata": {},
   "source": [
    "#to work out better in final EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b086ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_check(df_network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9707f2",
   "metadata": {},
   "source": [
    "In the network file we find 1446539 rows corresponding to all pipes, this is the master file for pipes.\n",
    "We find the most frequent Town is Madrid but the most frequent province is Barcelona. \n",
    "There are a total of 1972 Spanish towns in this dataset and 38 provinces\n",
    "We are missing the Basque Country, Ceuta, Melilla, the Canary Islands and Balearic Islands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1086cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar plot with the number of pipes per Town focusing in Madrid and Barcelona  in df_network\n",
    "df_network.groupby('Town')['PipeId'].count().sort_values(ascending=False).head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68359ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a pie chart with the number of pipes per Province in df_network \n",
    "df_network.groupby('Province')['PipeId'].count().sort_values(ascending=False).plot(kind='pie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aa92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(df_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d538de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join Inspection and network datasets on PipeId to create our TRAIN dataset\n",
    "train = pd.merge(df_inspection, df_network, on='PipeId', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3126f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c34bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inspection.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abaa644f",
   "metadata": {},
   "source": [
    "JF: There are 18K null values that do not have information in the df_network so our TRAIN data set is now bigger and we will need to deal with the nulls further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d89960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count null values in train dataset\n",
    "train.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Pedro\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\\Datathon-Rules-and-Documentation\")\n",
    "\n",
    "# Loading inspection data\n",
    "df_sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af43c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Juan\n",
    "#os.chdir(r\"C:\\Users\\JuanHorrillo\\OneDrive - IE Students\\Documents\\Masters\\Sustainability\\Notebook\")\n",
    "\n",
    "# Loading inspection data\n",
    "#df_sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b001d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the Incidence values in test dataset\n",
    "df_sample['Incidence'].value_counts()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4966fd39",
   "metadata": {},
   "source": [
    "Test data set is also unbalanced "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b2abff",
   "metadata": {},
   "source": [
    "Creating the TEST df which will be used to test our model.  Thisis made of the sample submission with the columns from the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the unique PipeId from df_sample and a dataframe with the PipeId from df_network \n",
    "sample_unique = df_sample['PipeId'].unique()\n",
    "network_unique = df_network['PipeId'].unique()\n",
    "#check if all the values in sample_unique are in network_unique\n",
    "np.all(np.isin(sample_unique, network_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae915d97",
   "metadata": {},
   "source": [
    "JF: All the PipeId on the df_sample have a PipeId on the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b328379",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join both datasets on PipeId\n",
    "test = pd.merge(df_network , df_sample, on='PipeId', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf550849",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f803e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b15fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a08a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values in test dataset\n",
    "test_null = test.isnull().sum()    \n",
    "test_null.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "039632ca",
   "metadata": {},
   "source": [
    "JF: No null Values in TEST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ffbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset for altering the dataset after initial EDA\n",
    "train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c28dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the columns with null values in train_copy\n",
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me a head of train_copy with YearBuilt == 2020\n",
    "train_copy[train_copy['YearBuilt'] > 2020].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13accacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the PipeID that have YearBuilt equal to 2021 and InspectionYear equal to NaN\n",
    "train_copy[(train_copy['YearBuilt'] > 2020) & (train_copy['InspectionYear'].isnull())].shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8248cba6",
   "metadata": {},
   "source": [
    "JF: There are 2026 pipes that were built after 2020 in the train data set, these came from the network data set and will be considered new pipes that do not have inspection data yet. We will assume that the inspection year for those was the year of installation (you inspect them when you install them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9292bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me nulls for InspectionYear for every year in train_copy\n",
    "train_copy.groupby('InspectionYear')['InspectionYear'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac000975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a range in the train_copy for the YearBuilt column if the value is greater than 2020 and the InspectionYear is equal to Nan, change the value of InspectionYear to YearBuilt\n",
    "\n",
    "#for i in train_copy['YearBuilt']:\n",
    "#    if i > 2020:\n",
    "#        train_copy.loc[(train_copy['YearBuilt'] > 2020) & (train_copy['InspectionYear'].isnull()), 'InspectionYear'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e706a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me the YearBuilt values for the null vales in InspectionYear\n",
    "train_copy[train_copy['InspectionYear'].isnull()]['YearBuilt'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from train_copy the rows where InspectionYear is equal to NaN and yearBuilt is not null\n",
    "train_copy = train_copy[~((train_copy['InspectionYear'].isnull()) & (train_copy['YearBuilt'].notnull()))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5250b5a8",
   "metadata": {},
   "source": [
    "We do not have info for these pipes so I will remove them all as it is impossible to find out the year built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the PipeId in inspection dataset that YearBuilt is equal to 2020 and inspection year is equal to 2021\n",
    "train_copy[(train_copy['YearBuilt'] > 2020) & (train_copy['InspectionYear'] == 2021)].shape\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47ff5b1",
   "metadata": {},
   "source": [
    "#perform al pip installs in one go comment out if already installed\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install statsmodels\n",
    "!pip install plotly\n",
    "!pip install cufflinks\n",
    "!pip install squarify\n",
    "!pip install yellowbrick\n",
    "!pip install lazypredict\n",
    "!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230c51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.style.palettes import PALETTES, SEQUENCES, color_palette\n",
    "\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for EDA. Using the display() function to have  well-formatted tables. We are mainly using pandas to explore the datasets\n",
    "\n",
    "def dataset_description(df_target):\n",
    "\n",
    "    print('This is the Dataset shape: %s\\n' % (df_target.shape, ))\n",
    "    print('Dataset columns: %s\\n' % df_target.columns)\n",
    "\n",
    "    print('\\nColumns description:\\n')\n",
    "    display(df_target.info())\n",
    "    display(df_target.describe())  # describe the dataset\n",
    "\n",
    "    print('\\nNull values:\\n')\n",
    "    display(df_target.isnull().sum())  # Identify null values\n",
    "\n",
    "#function performing a quick check on df_inspection to have best of pandas functions separated by a line\n",
    "def quick_check(dataframe):\n",
    "    print('First 5 rows %s\\n')\n",
    "    print(dataframe.head(2))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe shape %s\\n')\n",
    "    print(dataframe.shape)\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe describe categorical %s\\n')\n",
    "    print(dataframe.describe(include=['O']))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe null values %s\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe value counts %s\\n')\n",
    "    print(dataframe.value_counts())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#stats function\n",
    "def stats(dataframe):\n",
    "    print('Dataframe correlation %s\\n')\n",
    "    print(dataframe.corr())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe covariance %s\\n')\n",
    "    print(dataframe.cov())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe skew %s\\n')\n",
    "    print(dataframe.skew())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe kurtosis %s\\n')\n",
    "    print(dataframe.kurt())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#create a function to normalize characters from a dataset's column in Spanish\n",
    "def normalize_characters(df, column):\n",
    "    df[column] = df[column].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace('á', 'a')\n",
    "    df[column] = df[column].str.replace('é', 'e')\n",
    "    df[column] = df[column].str.replace('í', 'i')\n",
    "    df[column] = df[column].str.replace('ó', 'o')\n",
    "    df[column] = df[column].str.replace('ú', 'u')\n",
    "    df[column] = df[column].str.replace('ñ', 'n')\n",
    "    df[column] = df[column].str.replace('ü', 'u')\n",
    "    df[column] = df[column].str.replace('ç', 'c')\n",
    "    df[column] = df[column].str.replace('(', '')\n",
    "    df[column] = df[column].str.replace(')', '')\n",
    "    df[column] = df[column].str.replace('\\'', '')\n",
    "    df[column] = df[column].str.replace('´', '')\n",
    "    df[column] = df[column].str.replace('`', '')\n",
    "    df[column] = df[column].str.replace('’', '')\n",
    "    return df\n",
    "\n",
    "#create function to change detypes in64 to int32 in a df\n",
    "def change_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88a7d05",
   "metadata": {},
   "source": [
    "# 2. Data Collection and Understanding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931606e2",
   "metadata": {},
   "source": [
    "The network.csv file contains a static picture of the gas pipeline network. Every row corresponds to a pipe and has a unique PipeId identifier. The table has 1.446.529 pipes. \n",
    "\n",
    "The columns describe relevant features of each pipe. The complete list is: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d409b09",
   "metadata": {},
   "source": [
    "## Inspection file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7ed6752",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Pedro\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\\Datathon-Rules-and-Documentation\") \n",
    "\n",
    "#Loading sample_submission\n",
    "df_sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_check(df_inspection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for MaintenanceId ZRV-00002121 in df_inspection\n",
    "df_inspection[df_inspection['MaintenanceId'] == 'ZRV-00002121']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the distribution of MaintenanceId accross the years for ZRV-00002121 in df_inspection\n",
    "df_inspection[df_inspection['MaintenanceId'] == 'ZRV-00002121'].groupby('InspectionYear')['MaintenanceId'].count().plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7fc912",
   "metadata": {},
   "source": [
    "Seems like not so many inspections were carried in 2010, we shall decide on whether to keep this dimension or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(df_inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram with many bins of MonthsLastRev in  df_inspection\n",
    "df_inspection['MonthsLastRev'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ccb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUAN we kill more than 40 months revision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09db2f99",
   "metadata": {},
   "source": [
    "#to work out better in final EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb9707f2",
   "metadata": {},
   "source": [
    "In the network file we find 1446539 rows corresponding to all pipes, this is the master file for pipes.\n",
    "We find the most frequent Town is Madrid but the most frequent province is Barcelona. \n",
    "There are a total of 1972 Spanish towns in this dataset and 38 provinces\n",
    "We are missing the Basque Country, Ceuta, Melilla, the Canary Islands and Balearic Islands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1086cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a bar plot with the number of pipes per Town focusing in Madrid and Barcelona  in df_network\n",
    "df_network.groupby('Town')['PipeId'].count().sort_values(ascending=False).head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574aa92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(df_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3126f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abaa644f",
   "metadata": {},
   "source": [
    "JF: There are 18K null values that do not have information in the df_network so our TRAIN data set is now bigger and we will need to deal with the nulls further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8bcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored for Pedro\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\\Datathon-Rules-and-Documentation\")\n",
    "\n",
    "# Loading inspection data\n",
    "df_sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b001d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the Incidence values in test dataset\n",
    "df_sample['Incidence'].value_counts()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b2abff",
   "metadata": {},
   "source": [
    "Creating the TEST df which will be used to test our model.  Thisis made of the sample submission with the columns from the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae915d97",
   "metadata": {},
   "source": [
    "JF: All the PipeId on the df_sample have a PipeId on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join both datasets on PipeId\n",
    "test = pd.merge(df_network , df_sample, on='PipeId', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f803e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a08a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values in test dataset\n",
    "test_null = test.isnull().sum()    \n",
    "test_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ffbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset for altering the dataset after initial EDA\n",
    "train_copy = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b799ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me a head of train_copy with YearBuilt == 2020\n",
    "train_copy[train_copy['YearBuilt'] > 2020].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8248cba6",
   "metadata": {},
   "source": [
    "JF: There are 2026 pipes that were built after 2020 in the train data set, these came from the network data set and will be considered new pipes that do not have inspection data yet. We will assume that the inspection year for those was the year of installation (you inspect them when you install them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9292bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me nulls for InspectionYear for every year in train_copy\n",
    "train_copy.groupby('InspectionYear')['InspectionYear'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667145ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show me a head of train_copy with YearBuilt higher than InspectionYear and count them\n",
    "train_copy[(train_copy['YearBuilt'] > train_copy['InspectionYear'])].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e372052",
   "metadata": {},
   "source": [
    "# Feature Engineering on combined dataset\n",
    "\n",
    "As a result of merging both datasets we now have pipeline duplicates per each maintenace_id operation. Before getting rid of the duplicates, we want to engineer some metrics interesting to the model such as number of operations, number of incidents and average risk based on severity*incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column counting the number of inspections (MaintenanceId) per PipeId\n",
    "train_copy['No_Inspections'] = train_copy.groupby('PipeId')['MaintenanceId'].transform('count')\n",
    "#aggregate the number of Incidents per pipe in a new column and place it in the fourth position\n",
    "train_copy['No_Incidents'] = train_copy.groupby('PipeId')['Incidence'].transform('sum')\n",
    "#place the new columns in the third position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('No_Incidents'))\n",
    "train_copy = train_copy[['PipeId', 'MaintenanceId', 'No_Inspections', 'No_Incidents', 'InspectionYear', 'InspectionDate',\n",
    "       'MonthsLastRev', 'Severity', 'Incidence', 'Province', 'Town',\n",
    "       'YearBuilt', 'Material', 'GasType', 'Diameter', 'Length', 'Pressure',\n",
    "       'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n",
    "#show head of rows only where No_Incidents is greater than 0\n",
    "train_copy[train_copy['No_Incidents'] > 2].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column named average_severity_pipe that calculates the average severity per pipe aggregating all severities\n",
    "train_copy['average_severity_pipe'] = train_copy.groupby('PipeId')['Severity'].transform('mean')\n",
    "#show head of rows only where mean has a decimal value\n",
    "train_copy[train_copy['average_severity_pipe'] % 1 != 0].head(10)\n",
    "#place the new column in 7th position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('average_severity_pipe'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear',\n",
    "       'InspectionDate', 'MonthsLastRev', 'average_severity_pipe','Severity', 'Incidence', 'Province',\n",
    "       'Town', 'YearBuilt', 'Material', 'GasType', 'Diameter', 'Length',\n",
    "       'Pressure', 'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column taking average severity and number of total incidences per pipe multiplying them and naming it as risk_(s*i)\n",
    "train_copy['relative_risk'] = train_copy['average_severity_pipe'] * train_copy['No_Incidents'] \n",
    "#position the new column in the 7th position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('relative_risk'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear',\n",
    "       'InspectionDate', 'MonthsLastRev','relative_risk', 'average_severity_pipe', 'Severity',\n",
    "       'Incidence', 'Province', 'Town', 'YearBuilt', 'Material', 'GasType',\n",
    "       'Diameter', 'Length', 'Pressure', 'NumConnections',\n",
    "       'NumConnectionsUnder', 'BoolBridle']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bd590dc",
   "metadata": {},
   "source": [
    "This new column allows us to have a measure of risk by adding the mean aggregate of severity for an specific pipe and the aggregate number of incidents that we just created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89039b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show head of rows only where Risk_S*I is greater than 0\n",
    "train_copy[train_copy['relative_risk'] > 12].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "#show a treemap when relative_risk is greater than 12 showing the Material and the age of construction\n",
    "fig = px.treemap(train_copy[train_copy['relative_risk'] > 9], path=['Material', 'YearBuilt'], values='relative_risk')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83077fd1",
   "metadata": {},
   "source": [
    "PE and FD tendo to fail more when getting closer to 30 years old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0574d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show a treemap when relative_risk is greater than 12 showing the Provinces and Towns\n",
    "fig = px.treemap(train_copy[train_copy['relative_risk'] > 9], path=['Province', 'Town'], values='relative_risk')\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57c6a55b",
   "metadata": {},
   "source": [
    "Based on risk, Madrid, Barcelona, Valencia, Girona and Sevilla should be studied more in detail when elaborating a maintenance plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is the last inspection year for material FD\n",
    "train_copy[train_copy['Material'] == 'FD']['InspectionYear'].min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a12e23c4",
   "metadata": {},
   "source": [
    "We can see that  for a high risk factor, the most common materials are PE and FD with a mean of year built in the 90s. The pipes do not seem to be abandoned as the last year of inspection seems to be 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column taking Risk_S*I and dividing it by No_Inspections naming it as Risk_S*I/Inspections and placing it in 6th position\n",
    "train_copy['preventive_maintenance_rate'] = train_copy['relative_risk'] / train_copy['No_Inspections']\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('preventive_maintenance_rate'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear', 'preventive_maintenance_rate',\n",
    "       'InspectionDate', 'MonthsLastRev', 'relative_risk',\n",
    "       'average_severity_pipe', 'Severity', 'Incidence', 'Province', 'Town',\n",
    "       'YearBuilt', 'Material', 'GasType', 'Diameter', 'Length', 'Pressure',\n",
    "       'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n",
    "\n",
    "#show head of rows only where Risk_S*I/Inspections is greater than 0\n",
    "train_copy[train_copy['preventive_maintenance_rate'] > 0].head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "267f3c5b",
   "metadata": {},
   "source": [
    "In this new column we can see a dimension of how good it has been previous preventive maintenance  by dividing the relative risk factor (mean average risk per pipe*No_incidents) / No_inspections.\n",
    "The higher this index, the less maintenance it has undergone, hence the risk is not being mitigated on an active basis\n",
    "This graph can help us allocate better the maintenance in our business case using the Paretto rule \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph a treemap showing the Province and Towns where preventive_maintenance_rate is higher than 2\n",
    "fig = px.treemap(train_copy[train_copy['preventive_maintenance_rate'] > 2], path=['Province', 'Town'], values='preventive_maintenance_rate')\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5021536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column named probability with No_Incidents divided by Inspections\n",
    "train_copy['Probability_rate'] = train_copy['No_Incidents'] / train_copy['No_Inspections']\n",
    "#place column in 3rd position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('Probability_rate'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear', 'Probability_rate',\n",
    "       'preventive_maintenance_rate', 'InspectionDate', 'MonthsLastRev',\n",
    "       'relative_risk', 'average_severity_pipe', 'Severity', 'Incidence',\n",
    "       'Province', 'Town', 'YearBuilt', 'Material', 'GasType', 'Diameter',\n",
    "       'Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder',\n",
    "       'BoolBridle']]\n",
    "#show head of rows only where Probability is greater than 0\n",
    "train_copy[train_copy['Probability_rate'] > 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242452bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph a treemap showing the Province and Towns where Probability is higher than 4\n",
    "fig = px.treemap(train_copy[train_copy['Probability_rate'] > .5], path=['Province', 'Town'], values='Probability_rate')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "745f1a83",
   "metadata": {},
   "source": [
    "Having created risk and probability dimensions, we can compute a matrix to be assigned to the provinces for prioritization\n",
    "The matrix should include high risk, high probability to low risk and low probability    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ed549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a matrix with columns relative_risk and probability_rate for each province\n",
    "train_copy.groupby('Province')[['relative_risk', 'Probability_rate']].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a matrix with columns relative_risk and probability_rate for each pipe material\n",
    "train_copy.groupby('Material')[['relative_risk', 'Probability_rate']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd8501",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column with the Average of MonthsLastRev grouping per PipeId \n",
    "train_copy['Average_MonthsLastRev'] = train_copy.groupby('PipeId')['MonthsLastRev'].transform('mean')\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('Average_MonthsLastRev'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear',\n",
    "       'Probability_rate', 'preventive_maintenance_rate', 'InspectionDate', 'Average_MonthsLastRev',\n",
    "       'MonthsLastRev', 'relative_risk', 'average_severity_pipe', 'Severity',\n",
    "       'Incidence', 'Province', 'Town', 'YearBuilt', 'Material', 'GasType',\n",
    "       'Diameter', 'Length', 'Pressure', 'NumConnections',\n",
    "       'NumConnectionsUnder', 'BoolBridle']]\n",
    "train_copy.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88769df2",
   "metadata": {},
   "source": [
    "With this new column we know if the pipe is being inspected frewuently or not taking as a thresold 24 months. We create a ne column that takes this threshold into consideration and creates a boolean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column named pipe_inspected_frequently with a value of 1 if pipe has an Average_MonthsLastRev of less or equal than 24\n",
    "train_copy['pipe_inspected_frequently'] = np.where(train_copy['Average_MonthsLastRev'] <= 24, 1, 0)\n",
    "#place column in 7th position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('pipe_inspected_frequently'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear',\n",
    "         'Probability_rate', 'preventive_maintenance_rate', 'pipe_inspected_frequently', 'InspectionDate', 'Average_MonthsLastRev',\n",
    "         'MonthsLastRev', 'relative_risk', 'average_severity_pipe', 'Severity',\n",
    "            'Incidence', 'Province', 'Town', 'YearBuilt', 'Material', 'GasType',\n",
    "            'Diameter', 'Length', 'Pressure', 'NumConnections',\n",
    "            'NumConnectionsUnder', 'BoolBridle']]\n",
    "#delete column Average_MonthsLastRev\n",
    "train_copy.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac174fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column named age_pipe_inspection with the difference between InspectionYear and YearBuilt and placing it in 10 th position\n",
    "train_copy['Age_pipe_at_inspection'] = train_copy['InspectionYear'] - train_copy['YearBuilt']\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('Age_pipe_at_inspection'))\n",
    "train_copy = train_copy[['PipeId', 'No_Inspections', 'No_Incidents', 'InspectionYear',\n",
    "       'Probability_rate', 'preventive_maintenance_rate', 'Age_pipe_at_inspection',\n",
    "       'pipe_inspected_frequently', 'InspectionDate', 'Average_MonthsLastRev',\n",
    "       'MonthsLastRev', 'relative_risk', 'average_severity_pipe', 'Severity',\n",
    "       'Incidence', 'Province', 'Town', 'YearBuilt', 'Material', 'GasType',\n",
    "       'Diameter', 'Length', 'Pressure', 'NumConnections',\n",
    "       'NumConnectionsUnder', 'BoolBridle']]\n",
    "#head of pipes with Age_pipe_at_inspection greater than 0\n",
    "train_copy[train_copy['Age_pipe_at_inspection'] > 0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe Diameter on train_copy in mm\n",
    "train_copy['Diameter'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca5501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['Length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41349dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide all values in Diameter by 1000 to convert to meters\n",
    "train_copy['Diameter'] = train_copy['Diameter'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new variable called aspect that is equal to pressure divided by diameter multiplied by length\n",
    "train_copy['aspect']=(train_copy['Pressure']/train_copy['Diameter'])/train_copy['Length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column that divides the diameter by the pressure and name it Relative Thickness\n",
    "train_copy['Relative_Thickness'] = train_copy['Diameter'] / train_copy['Pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5971ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column named pipe_area that multiplies diameter by lenght by pi\n",
    "\n",
    "train_copy['pipe_area'] = train_copy['Diameter'] * train_copy['Length'] * 3.1416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column named area_connection that divides pipe_area by NumConnections\n",
    "train_copy['area_connection'] = train_copy['NumConnections']/ train_copy['pipe_area'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c589b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column named incidence_area that divides incidence by pipe_area\n",
    "train_copy['incidence_area'] = train_copy['Incidence'] / train_copy['pipe_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a boolean column named connection_bool that is 1 if NumConnections is greater than 1\n",
    "train_copy['connection_bool'] = np.where(train_copy['NumConnections'] > 1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform Inspection Date to datetime format\n",
    "train_copy['InspectionDate'] = pd.to_datetime(train_copy['InspectionDate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6102d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot encode severity_incidence column in the train_copy dataframe\n",
    "train_copy = pd.get_dummies(train_copy, columns=['Severity'], prefix = ['Severity'])\n",
    "train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change name of Severity_Incidence_1 column to Severity_low\n",
    "train_copy = train_copy.rename(columns={'Severity_1.0': 'Severity_high'})\n",
    "#change name of Severity_Incidence_2 column to Severity_medium\n",
    "train_copy = train_copy.rename(columns={'Severity_2.0': 'Severity_medium'})\n",
    "#change name of Severity_Incidence_3 column to Severity_high\n",
    "train_copy = train_copy.rename(columns={'Severity_3.0': 'Severity_low'})\n",
    "#drop Severity_4.0 column\n",
    "train_copy = train_copy.drop(['Severity_4.0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d43d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy['BoolBridle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Boolbride into  boolean variable\n",
    "def boolbridle(x):\n",
    "    return 1 if x == 'True' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8031e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function on dataset\n",
    "train_copy['BoolBridle'] = train_copy['BoolBridle'].apply(lambda x: boolbridle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce8701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot encode GasType column in the train_copy dataframe subset\n",
    "train_copy = pd.get_dummies(train_copy, columns=['GasType'], prefix = ['GasType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete GasType_Gas propano column\n",
    "train_copy = train_copy.drop(['GasType_Gas propano'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change name of GasType_Gas natural column to gas_natural\n",
    "train_copy = train_copy.rename(columns={'GasType_Gas natural': 'gas_natural'})\n",
    "train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe968ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how much RAM is being used\n",
    "import psutil\n",
    "psutil.virtual_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cc8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_copy['Material'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map df_combined Material column to the names of materials\n",
    "train_copy['Material'] = train_copy['Material'].map({'PE': 'Polyethylene', 'AO': 'Acrylonitrile-Butadiene-Styrene', 'FD': 'Fiberglass-Reinforced Plastic', \n",
    "    'FG': 'Fiberglass', 'PN': 'Polypropylene', 'PA': 'Polyamide', 'FO': 'Flexible Polyolefin', 'FI': 'Flexible Polyvinyl Chloride', 'CU': 'Copper', \n",
    "    'PV': 'Polyvinylidene Fluoride', 'ZD': 'Zinc-Coated Steel', 'ZA': 'Zinc-Aluminum', 'CP': 'Cast Iron', 'CS': 'Cast Steel', \n",
    "    'ZC': 'Zinc-Coated Steel', 'ZM': 'Zinc-Magnesium','ZN': 'Zinc', 'AL': 'Aluminum', 'ZP': 'Zinc-Coated Steel', 'ZF': 'Zinc-Aluminum-Magnesium'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot enconde Material column in the train_copy dataframe subset\n",
    "train_copy = pd.get_dummies(train_copy, columns=['Material'])\n",
    "train_copy = train_copy.drop(['Material_Fiberglass', 'Material_Zinc-Coated Steel', 'Material_Polyvinylidene Fluoride','Material_Flexible Polyolefin', 'Material_Flexible Polyvinyl Chloride', 'Material_Polyamide'], axis=1)\n",
    "train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554afd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create polinomial features in train_copy for Diameter, Length, Pressure\n",
    "poly = PolynomialFeatures(2)\n",
    "train_copy['Diameter2'] = poly.fit_transform(train_copy[['Diameter']])[:,2]\n",
    "train_copy['Length2'] = poly.fit_transform(train_copy[['Length']])[:,2]\n",
    "train_copy['Pressure2'] = poly.fit_transform(train_copy[['Pressure']])[:,2]\n",
    "train_copy.head(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee189e5b",
   "metadata": {},
   "source": [
    "# Dropping \n",
    "\n",
    "Mkaing df lighter just before joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.shape#delete the rows with YearBuilt higher than InspectionYear\n",
    "train_copy = train_copy.drop(train_copy[(train_copy['YearBuilt'] > train_copy['InspectionYear'])].index)\n",
    "train_copy.drop('InspectionDate', axis=1, inplace=True)\n",
    "#show null values in train_copy\n",
    "train_copy.isnull().sum()\n",
    "    \n",
    "#delete nulls from train_copy\n",
    "train_copy = train_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44776e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the rows with YearBuilt higher than InspectionYear\n",
    "train_copy = train_copy.drop(train_copy[(train_copy['YearBuilt'] > train_copy['InspectionYear'])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa35d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show null values in train_copy\n",
    "train_copy.isnull().sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls from train_copy\n",
    "train_copy = train_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del df_inspection\n",
    "del df_network\n",
    "del network_unique\n",
    "del train\n",
    "del df_sample_submission\n",
    "del df_sample\n",
    "del sample_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c6610",
   "metadata": {},
   "source": [
    "# Adding a new dataset\n",
    "We want to extract value of two categorical variables, Town and Province but the way we have them now they are useless.\n",
    "\n",
    "We will add a new dataset to join and extract the surface of each town as well as the comunidad autonoma to group by accordingly in another column the number of towns and afterwards hot encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5189a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count towns populating Town column\n",
    "train_copy['Town'].value_counts()\n",
    "train_copy['Town'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e80bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\\base\\complementary_datasets\")\n",
    "\n",
    "# Loading provincias dataset\n",
    "provincias = pd.read_excel('promedio_tiempo_provincia_anual.xlsx')\n",
    "\n",
    "#Loading municipios dataset\n",
    "#df_mun = pd.read_excel('list-mun-2012.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read population\n",
    "population = pd.read_excel('pobmun20.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf01f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "population.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_characters(population, 'Province')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_characters(population, 'Municipio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change Municipio column with Town column\n",
    "population = population.rename(columns={'Municipio': 'Town'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "provincias.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_characters(provincias, 'Province')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_characters(train_copy, 'Province')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70169a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join train_copy and provincias on Province\n",
    "train_copy = train_copy.merge(provincias, on='Province', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fddbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count nulls in train_copy\n",
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad34631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare array for column Province in train_copy and provincias and compute the set difference\n",
    "np.setdiff1d(train_copy['Province'].unique(), provincias['Province'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c07b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#place Province and Town Columns at the end of the dataframe\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('Province'))\n",
    "cols.pop(cols.index('Town'))\n",
    "train_copy = train_copy[cols+['Province','Town']]\n",
    "train_copy.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3252c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join train_copy and population on Town considering the best match\n",
    "train_copy = train_copy.merge(population, on='Town', how='left')\n",
    "\n",
    "#count nulls in train_copy\n",
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fuzzyjoin\n",
    "#pip install fuzzymatcher\n",
    "#pip install fuzzy_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzymatcher import link_table, fuzzy_left_join\n",
    "import fuzzyjoin\n",
    "import difflib\n",
    "import fuzzy_pandas as fpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fuzzypandas to merge train_copy and population on Town \n",
    "train_copy = fpd.fuzzy_merge(train_copy, population, left_on='Town', right_on='Town', method='levenshtein', threshold=0.6, keep='match', join=\"left-outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping df_mun from memory to free RAM\n",
    "del df_mun\n",
    "del train\n",
    "del df_inspection\n",
    "del df_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98da0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb53ca6",
   "metadata": {},
   "source": [
    "# Outlier section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete outliers in Year_Built column < 1960\n",
    "train_copy = train_copy[train_copy['YearBuilt'] > 1960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the distribution of YearBuilt\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(train_copy['YearBuilt'], color='blue', bins=100, hist_kws={'alpha': 0.4});\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate values higher than 40 in MonthsLastRev column\n",
    "train_copy = train_copy[train_copy['MonthsLastRev'] < 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the distribution of MonthsLastRev\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(train_copy['MonthsLastRev'], color='blue', bins=100, hist_kws={'alpha': 0.4});\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use describe function of monthslastrev column to see the distribution of values\n",
    "train_copy['MonthsLastRev'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b4b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate values higher than 400 in diameter column\n",
    "train_copy = train_copy[train_copy['Diameter'] < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the distribution of Diameter\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot(train_copy['Diameter'], color='blue', bins=100, hist_kws={'alpha': 0.4});\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cddf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Age_pipe_at_inspection','Length','YearBuilt','InspectionYear'] # one or more\n",
    "\n",
    "Q1 = train_copy[cols].quantile(0.25)\n",
    "Q3 = train_copy[cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "train_copy = train_copy[~((train_copy[cols] < (Q1 - 1.5 * IQR)) |(train_copy[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph histogram of MonthsLastRev column\n",
    "train_copy['MonthsLastRev'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f79d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot column Length\n",
    "train_copy['Length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of Length column\n",
    "train_copy['Length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f30d4011",
   "metadata": {},
   "source": [
    "# Dropping section\n",
    "Dropping will be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the rows with YearBuilt higher than InspectionYear\n",
    "train_copy = train_copy.drop(train_copy[(train_copy['YearBuilt'] > train_copy['InspectionYear'])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd158e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.drop('InspectionDate', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show null values in train_copy\n",
    "train_copy.isnull().sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls from train_copy\n",
    "train_copy = train_copy.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b414cf",
   "metadata": {},
   "source": [
    "# Performing dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a pca to reduce the number of columns in train_copy\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(train_copy)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "#show the columns of train_copy\n",
    "train_copy.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1765b9ba",
   "metadata": {},
   "source": [
    "# Exporting the new dataset for a next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518deb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\")\n",
    "#export the dataframe to a csv file\n",
    "train_copy.to_csv('train_consolidated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda55b3",
   "metadata": {},
   "source": [
    "# Plotting EDA for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas profiling on the train_copy dataframe\n",
    "profile = ProfileReport(train_copy, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n",
    "profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45701319",
   "metadata": {},
   "source": [
    "# The Archive Section\n",
    "Just in case we regret....\n",
    "NOT DELETED JUST IN CASE NEEDED, HIGHLY CONFIDENT WE DONT NEED THESE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596ed67",
   "metadata": {},
   "source": [
    "#aggregate the number of Age_pipe_inspection in a new column\n",
    "train_copy['Aggregate_pipe_age_inspection'] = train_copy.groupby('PipeId')['Age_pipe_inspection'].transform('sum')\n",
    "#place column in 10th position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('Aggregate_pipe_age_inspection'))\n",
    "train_copy = train_copy[['PipeId', 'MaintenanceId', 'Inspections', 'Probability_incidence', 'Risk_S*I/Inspections', 'Average_MonthsLastRev',\n",
    "    'Age_pipe_inspection', 'Aggregate_pipe_age_inspection', 'No_Incidents', 'Risk_S*I', 'average_severity', 'MonthsLastRev', 'Severity', 'Incidence', 'Province',\n",
    "    'Town','YearBuilt', 'InspectionYear', 'InspectionDate','Material', 'GasType', 'Diameter', 'Length', 'Pressure',\n",
    "    'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n",
    "train_copy.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5637c12",
   "metadata": {},
   "source": [
    "#divide the Aggregate_pipe_age_inspection by the count of Incidence when is 1 and create new column with the average_age_pipe_inspection_when_incidence saving it in 10th position\n",
    "train_copy['average_age_pipe_inspection_when_incidence'] = train_copy['Aggregate_pipe_age_inspection'] / train_copy.groupby('PipeId')['Incidence'].transform('count')\n",
    "#place column in 10th position\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('average_age_pipe_inspection_when_incidence'))\n",
    "train_copy = train_copy[['PipeId', 'MaintenanceId', 'Inspections', 'Probability_incidence', 'Risk_S*I/Inspections', 'Average_MonthsLastRev',\n",
    "    'Age_pipe_inspection', 'Aggregate_pipe_age_inspection', 'average_age_pipe_inspection_when_incidence', 'No_Incidents', 'Risk_S*I', 'average_severity', 'MonthsLastRev', 'Severity', 'Incidence', 'Province',\n",
    "    'Town','YearBuilt', 'InspectionYear', 'InspectionDate','Material', 'GasType', 'Diameter', 'Length', 'Pressure',\n",
    "    'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n",
    "#head of rows only where average_age_pipe_inspection_when_incidence when Incidence is 0\n",
    "train_copy[train_copy['Incidence'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fca87",
   "metadata": {},
   "source": [
    "#take PipeId column from df_submission dataframe and match it with PipeId column in train_copy dataframe to split train_copy into df_combined_train and df_combined_test. The test split should be the one with the higher number of rows\n",
    "df_combined_test = train_copy[train_copy['PipeId'].isin(df_submission['PipeId'])]\n",
    "df_combined_train = train_copy[~train_copy['PipeId'].isin(df_submission['PipeId'])]\n",
    "#Do not look back at the test set until you are ready to submit your predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6669e5",
   "metadata": {},
   "source": [
    "#take PipeId column from df_submission dataframe and match it with PipeId column in train_copy dataframe to split train_copy into df_combined_train and df_combined_test. The test split should be the one with the higher number of rows\n",
    "df_combined_trained = train_copy[train_copy['PipeId'].isin(df_submission['PipeId'])]\n",
    "df_combined_test = train_copy[~train_copy['PipeId'].isin(df_submission['PipeId'])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35f5d6a9",
   "metadata": {},
   "source": [
    "#normalize df_combined_train_sub dataframe but the booleans\n",
    "df_combined_train_sub_norm = df_combined_train_sub.copy()\n",
    "df_combined_train_sub_norm[['Inspections', 'No_Incidents', 'Risk_S*I/Inspections', 'MonthsLastRev', 'Risk_S*I', 'Severity',\n",
    "         'YearBuilt', 'Diameter', 'Length', 'Pressure', 'NumConnections','NumConnectionsUnder', 'TownCount']] = MinMaxScaler().fit_transform(df_combined_train_sub_norm[['Inspections', 'No_Incidents', 'Risk_S*I/Inspections', 'MonthsLastRev', 'Risk_S*I', 'Severity',\n",
    "            'YearBuilt', 'Diameter', 'Length', 'Pressure', 'NumConnections','NumConnectionsUnder', 'TownCount']])\n",
    "df_combined_train_sub_norm.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23e7186b",
   "metadata": {},
   "source": [
    "#Extract the day of the week from InspectionDate and place it in 5th position\n",
    "train_copy['InspectionDay'] = train_copy['InspectionDate'].dt.day_name()\n",
    "cols = list(train_copy.columns.values)\n",
    "cols.pop(cols.index('InspectionDay'))\n",
    "train_copy = train_copy[['PipeId', 'Inspections', 'No_Incidents', 'Risk_S*I/Inspections','leakage_estimate_factor','InspectionDay',\n",
    "    'InspectionYear', 'InspectionDate', 'MonthsLastRev', 'Risk_S*I','Severity','Incidence', 'Province', 'Town', 'YearBuilt', 'Material', 'GasType',\n",
    "    'Diameter', 'Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'BoolBridle']]\n",
    "train_copy.head(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "465c900b",
   "metadata": {},
   "source": [
    "#on the train_copy if YEarBuilt is equal to 2021 and InspectionYear is equal to NaN, fill the InspectionYear with 2021\n",
    "#train_copy.loc[(train_copy['YearBuilt'] > 2020) & (train_copy['InspectionYear'].isnull()), 'InspectionYear'] = 2021\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3e17e5",
   "metadata": {},
   "source": [
    "#do a range in the train_copy for the YearBuilt column if the value is greater than 2020 and the InspectionYear is equal to Nan, change the value of InspectionYear to YearBuilt\n",
    "\n",
    "for i in train_copy['YearBuilt']:\n",
    "    if i > 2020:\n",
    "        train_copy.loc[(train_copy['YearBuilt'] > 2020) & (train_copy['InspectionYear'].isnull()), 'InspectionYear'] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31cd550e",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "326e4f63ad54c217260fc7be1c53acea6ef3ea6cd7ac93b3b02195c6d8fa7cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
