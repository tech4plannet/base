{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6ba2d28",
   "metadata": {},
   "source": [
    "# Libraries used\n",
    "\n",
    "Running Kernel3.9.13 base anaconda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47ff5b1",
   "metadata": {},
   "source": [
    "pip install squarify\n",
    "pip install yellowbrick\n",
    "pip install plotly\n",
    "pip install seaborn\n",
    "pip install lazypredict\n",
    "pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e469ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import os\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "#to enable the inline plotting\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#rfc using regularization and gridsearch to find the best parameters for the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.style.palettes import PALETTES, SEQUENCES, color_palette\n",
    "\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for EDA. Using the display() function to have  well-formatted tables. We are mainly using pandas to explore the datasets\n",
    "\n",
    "def dataset_description(df_target):\n",
    "\n",
    "    print('This is the Dataset shape: %s\\n' % (df_target.shape, ))\n",
    "    print('Dataset columns: %s\\n' % df_target.columns)\n",
    "\n",
    "    print('\\nColumns description:\\n')\n",
    "    display(df_target.info())\n",
    "    display(df_target.describe())  # describe the dataset\n",
    "\n",
    "    print('\\nNull values:\\n')\n",
    "    display(df_target.isnull().sum())  # Identify null values\n",
    "\n",
    "#function performing a quick check on df_inspection to have best of pandas functions separated by a line\n",
    "def quick_check(dataframe):\n",
    "    print('First 5 rows %s\\n')\n",
    "    print(dataframe.head(2))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe shape %s\\n')\n",
    "    print(dataframe.shape)\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe describe categorical %s\\n')\n",
    "    print(dataframe.describe(include=['O']))\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe null values %s\\n')\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe value counts %s\\n')\n",
    "    print(dataframe.value_counts())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#stats function\n",
    "def stats(dataframe):\n",
    "    print('Dataframe correlation %s\\n')\n",
    "    print(dataframe.corr())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe covariance %s\\n')\n",
    "    print(dataframe.cov())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe skew %s\\n')\n",
    "    print(dataframe.skew())\n",
    "    print(\"=====================================\")\n",
    "    print('Dataframe kurtosis %s\\n')\n",
    "    print(dataframe.kurt())\n",
    "    print(\"=====================================\")\n",
    "\n",
    "#create a function to normalize characters from a dataset's column in Spanish\n",
    "def normalize_characters(df, column):\n",
    "    df[column] = df[column].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace('á', 'a')\n",
    "    df[column] = df[column].str.replace('é', 'e')\n",
    "    df[column] = df[column].str.replace('í', 'i')\n",
    "    df[column] = df[column].str.replace('ó', 'o')\n",
    "    df[column] = df[column].str.replace('ú', 'u')\n",
    "    df[column] = df[column].str.replace('ñ', 'n')\n",
    "    df[column] = df[column].str.replace('ü', 'u')\n",
    "    df[column] = df[column].str.replace('ç', 'c')\n",
    "    df[column] = df[column].str.replace('(', '')\n",
    "    df[column] = df[column].str.replace(')', '')\n",
    "    df[column] = df[column].str.replace('\\'', '')\n",
    "    df[column] = df[column].str.replace('´', '')\n",
    "    df[column] = df[column].str.replace('`', '')\n",
    "    df[column] = df[column].str.replace('’', '')\n",
    "    return df.head(2)\n",
    "\n",
    "#create function to change detypes in64 to int32 in a df\n",
    "def change_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "def outlier_function(df, col_name):\n",
    "    \"\"\" this function detects first and third quartile and interquartile range for a given column of a dataframe\n",
    "    then calculates upper and lower limits to determine outliers conservatively\n",
    "    returns the number of lower and uper limit and number of outliers respectively\"\"\"\n",
    "    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n",
    "    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n",
    "    IQR = third_quartile - first_quartile\n",
    "                        \n",
    "    upper_limit = third_quartile+(3*IQR)\n",
    "    lower_limit = first_quartile-(3*IQR)\n",
    "    outlier_count = 0\n",
    "                    \n",
    "    for value in df[col_name].tolist():\n",
    "        if (value < lower_limit) | (value > upper_limit):\n",
    "            outlier_count +=1\n",
    "        else:\n",
    "            pass\n",
    "    return lower_limit, upper_limit, outlier_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e28e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all print outputs when using a function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ebce254",
   "metadata": {},
   "source": [
    "# Importing previous dataset with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#point to the folder where the data is stored\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\")\n",
    "\n",
    "# Loading combined_mod dataset\n",
    "train_consolidated = pd.read_csv('train_consolidated_with_outliers_2202.csv')\n",
    "sample_submission = pd.read_csv('submission_for_outliers_220223.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the same order for the columns in train_consolidated and sample_submission\n",
    "train_consolidated = train_consolidated[sample_submission.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check nulls in sample_submission \n",
    "sample_submission.isnull().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bdde928",
   "metadata": {},
   "source": [
    "## Filling Nulls in sample submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57919d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nulls in No_Inspectio with 1\n",
    "sample_submission['No_Inspections'].fillna(1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nulls in average_severity_pipe  with the mean of the column\n",
    "sample_submission['average_severity_pipe'].fillna(sample_submission['average_severity_pipe'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nulls in relative_risk by multyplying No_Incidences by  average_severity_pipe\n",
    "sample_submission['relative_risk'] = sample_submission['No_Previous_Incidences'] * sample_submission['average_severity_pipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nulls in preventive_maintenance_rate by dividing relative_risk by No_Inspections\n",
    "sample_submission['preventive_maintenance_rate'] = sample_submission['relative_risk'] / sample_submission['No_Inspections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nulls in Probability_rate with No_Previous_Incidences divided by No_Inspections\n",
    "sample_submission['Probability_rate'] = sample_submission['No_Previous_Incidences'] / sample_submission['No_Inspections']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f6c0c23",
   "metadata": {},
   "source": [
    "## Dtypes in sample submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d19392",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Province, Town from sample_submission\n",
    "sample_submission.drop(['Province', 'Town'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b415b9",
   "metadata": {},
   "source": [
    "## Standarizing data in sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import min max scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use minmax scaler to scale these columns in sample_submission: \n",
    "# 'YearBuilt', 'Diameter','Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'aspect', 'Relative_Thickness', 'pipe_area', 'Total_Connections', 'area_connection', 'Diameter2', 'Length2', 'Pressure2', 'Average yearly temperature (°C)', 'Min. Temperature (°C)', 'Max. Temperature (°C)', 'Yearly Rainfall (mm)', 'Average year Humidity (%)', 'Rainy days per year (days)', 'Yearly Sun Hours (hours)','No_Inspections', 'average_severity_pipe', 'Average_MonthsLastRev',\n",
    "#       'No_Previous_Incidences', 'relative_risk', 'preventive_maintenance_rate',\n",
    "#       'Probability_rate', 'No_Inspections', 'average_severity_pipe',\n",
    "#       'relative_risk', 'preventive_maintenance_rate', 'Probability_rate'\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "sample_submission[['YearBuilt', 'Diameter','Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'aspect', 'Relative_Thickness', 'pipe_area', 'Total_Connections', 'area_connection', 'Diameter2', 'Length2', 'Pressure2', 'Average yearly temperature (°C)', 'Min. Temperature (°C)', 'Max. Temperature (°C)', 'Yearly Rainfall (mm)', 'Average year Humidity (%)', 'Rainy days per year (days)', 'Yearly Sun Hours (hours)','No_Inspections', 'average_severity_pipe', 'Average_MonthsLastRev',\n",
    "       'No_Previous_Incidences', 'relative_risk', 'preventive_maintenance_rate',\n",
    "       'Probability_rate', 'No_Inspections', 'average_severity_pipe',\n",
    "       'relative_risk', 'preventive_maintenance_rate', 'Probability_rate', 'InspectionYear',  'MonthsLastRev','Age_pipe_at_inspection']] = scaler.fit_transform(sample_submission[['YearBuilt', 'Diameter','Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'aspect', 'Relative_Thickness', 'pipe_area', 'Total_Connections', 'area_connection', 'Diameter2', 'Length2', 'Pressure2', 'Average yearly temperature (°C)', 'Min. Temperature (°C)', 'Max. Temperature (°C)', 'Yearly Rainfall (mm)', 'Average year Humidity (%)', 'Rainy days per year (days)', 'Yearly Sun Hours (hours)','No_Inspections', 'average_severity_pipe', 'Average_MonthsLastRev',\n",
    "       'No_Previous_Incidences', 'relative_risk', 'preventive_maintenance_rate',\n",
    "       'Probability_rate', 'No_Inspections', 'average_severity_pipe',\n",
    "       'relative_risk', 'preventive_maintenance_rate', 'Probability_rate', 'InspectionYear', 'MonthsLastRev','Age_pipe_at_inspection']])\n",
    "sample_submission.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac99dd92",
   "metadata": {},
   "source": [
    "## Correlation matrix for sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grpah correlation matrix in sample_submission\n",
    "corr_matrix = sample_submission.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.corr()['Incidence'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ac6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP Total_Connections IN sample_submission\n",
    "sample_submission.drop(['Total_Connections'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba35cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', mask=np.triu(corr_matrix, k=1))\n",
    "plt.title('Correlation matrix for sample_submission')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11f1b120",
   "metadata": {},
   "source": [
    "We are not going to drop any variables as in now just for testing purposes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750657f4",
   "metadata": {},
   "source": [
    "# Checking train file\n",
    "## Nulls in train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cd7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking nulls in train_consolidated\n",
    "train_consolidated.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a27b6c2d",
   "metadata": {},
   "source": [
    "## Dtypes in train_consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_consolidated.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f15828",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_consolidated.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d995ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop 'MaintenanceId', InspectionDate, Province, Province, 'Severity_high', 'Severity_medium', 'Severity_low'\n",
    "train_consolidated.drop(['Province', 'Town'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference in columns between train_consolidated and sample_submission\n",
    "set(train_consolidated.columns) - set(sample_submission.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e37f7957",
   "metadata": {},
   "source": [
    "## Standarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be84d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_consolidated[['YearBuilt', 'Diameter','Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'aspect', 'Relative_Thickness', 'pipe_area', 'Total_Connections', 'area_connection', 'Diameter2', 'Length2', 'Pressure2', 'Average yearly temperature (°C)', 'Min. Temperature (°C)', 'Max. Temperature (°C)', 'Yearly Rainfall (mm)', 'Average year Humidity (%)', 'Rainy days per year (days)', 'Yearly Sun Hours (hours)','No_Inspections', 'average_severity_pipe', 'Average_MonthsLastRev',\n",
    "       'No_Previous_Incidences', 'relative_risk', 'preventive_maintenance_rate',\n",
    "       'Probability_rate', 'No_Inspections', 'average_severity_pipe',\n",
    "       'relative_risk', 'preventive_maintenance_rate', 'Probability_rate', 'InspectionYear',  'MonthsLastRev','Age_pipe_at_inspection']] = scaler.fit_transform(train_consolidated[['YearBuilt', 'Diameter','Length', 'Pressure', 'NumConnections', 'NumConnectionsUnder', 'aspect', 'Relative_Thickness', 'pipe_area', 'Total_Connections', 'area_connection', 'Diameter2', 'Length2', 'Pressure2', 'Average yearly temperature (°C)', 'Min. Temperature (°C)', 'Max. Temperature (°C)', 'Yearly Rainfall (mm)', 'Average year Humidity (%)', 'Rainy days per year (days)', 'Yearly Sun Hours (hours)','No_Inspections', 'average_severity_pipe', 'Average_MonthsLastRev',\n",
    "       'No_Previous_Incidences', 'relative_risk', 'preventive_maintenance_rate',\n",
    "       'Probability_rate', 'No_Inspections', 'average_severity_pipe',\n",
    "       'relative_risk', 'preventive_maintenance_rate', 'Probability_rate', 'InspectionYear', 'MonthsLastRev','Age_pipe_at_inspection']])\n",
    "train_consolidated.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fffff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a subset for altering the dataset after initial EDA\n",
    "df_baseline = train_consolidated.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86211d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count values for Incidence\n",
    "df_baseline['Incidence'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef46f5eb",
   "metadata": {},
   "source": [
    "# Correlation in df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for df_baseline\n",
    "corr_matrix = df_baseline.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation numbers with target Incidence for Total_Connections', 'relative_risk', 'Probability_rate\n",
    "df_baseline.corr()['Incidence'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Total_Connections IN df_baseline\n",
    "df_baseline.drop(['Total_Connections'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', mask=np.triu(corr_matrix, k=1))\n",
    "plt.title('Correlation matrix for df_baseline')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f642e281",
   "metadata": {},
   "source": [
    "# Approach 0: Running model with all df (best score no split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d79d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename df_baseline as x\n",
    "X = df_baseline.drop('Incidence', axis=1)\n",
    "y = df_baseline['Incidence']\n",
    "X_test = sample_submission.drop('Incidence', axis=1)\n",
    "y_test = sample_submission['Incidence'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "StopAsyncIteration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d961fba7",
   "metadata": {},
   "source": [
    "# Approach 1: Stratifying with undersampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d1e24",
   "metadata": {},
   "source": [
    "#undersampling the dataset\n",
    "df_baseline_undersampling = df_baseline.groupby('Incidence').apply(lambda x: x.sample(df_baseline['Incidence'].value_counts().min(), random_state=42)).reset_index(drop=True)\n",
    "\n",
    "df_baseline_undersampling['Incidence'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162c16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf1abca0",
   "metadata": {},
   "source": [
    "# Approach 2 : SMOTE to tackle the unbalanced dataset problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57968ab",
   "metadata": {},
   "source": [
    "#using SMOTE to balance the dataset creating a subset dataset with target y = 'Incidence' and removing Incidence','MonthsLastRev','InspectionDay','PipeId', 'MaintenanceId', 'InspectionYear', 'InspectionDate',  'Province', 'Town', 'YearBuilt' for x\n",
    "X = df_baseline.drop(['PipeId','Province', 'Town'], axis=1)\n",
    "y = df_baseline['Incidence']\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "X_res.shape,y_res.shape\n",
    "\n",
    "#create a pandas dataframe with the new balanced dataset\n",
    "df_baseline_balanced = pd.DataFrame(X_res, columns=X.columns)\n",
    "df_baseline_balanced['Incidence'] = y_res\n",
    "df_baseline_balanced.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bec3ae",
   "metadata": {},
   "source": [
    "#using SMOTE to balance the dataset creating a subset dataset with target y = 'Incidence'\n",
    "X = df_baseline.drop('Incidence', axis=1)\n",
    "y = df_baseline['Incidence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "#count values for Incidence in y_train\n",
    "y_train.value_counts()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a4bc545",
   "metadata": {},
   "source": [
    "# Baseline approach\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0f2e5a9",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dtypes(df_baseline)\n",
    "change_dtypes(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_consolidated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d054e766",
   "metadata": {},
   "source": [
    "# Trying model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfc and cross validation using GridSearchCV to find best parameters and validate with a validation set with InspectionYear 2018 and 2019\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, verbose=2, class_weight='balanced_subsample', n_estimators=300, max_features='auto', max_depth=3, criterion='gini')\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3, scoring='roc_auc', n_jobs=-1, verbose=2, return_train_score=True)\n",
    "CV_rfc.fit(X, y)\n",
    "CV_rfc.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113b2bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a subset with Inspection Year 2018 and 2019 to use as validation set\n",
    "df_val = df_baseline[df_baseline['InspectionYear'].isin([2018, 2019])]\n",
    "df_val.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9be844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadeed28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest classifier with regularization and gridsearch to find the best parameters for the model\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth = 3, verbose=2, n_estimators=100, max_features='auto', min_samples_leaf=1, min_samples_split=2, class_weight='balanced' )\n",
    "rfc.fit(X, y)\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53529f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict_proba(X_test)\n",
    "predictions = predictions[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use model to predict on test set\n",
    "predictions = rfc.predict_proba(X_test)\n",
    "predictions = predictions[:, 1]\n",
    "\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop incidence in sample_submission\n",
    "sample_submission = sample_submission.drop('Incidence', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129af5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions for submission using the model\n",
    "predictions = rfc.predict_proba(sample_submission)\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554eec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add predictions to submission\n",
    "sample_submission ['Incidence'] = predictions\n",
    "\n",
    "#show submission\n",
    "sample_submission.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aab2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns except PipeId and Incidence in submission2 creating kaggle_submission\n",
    "kaggle_submission = sample_submission.drop(['YearBuilt', 'Diameter', 'Length', 'Pressure',\n",
    "       'NumConnections', 'NumConnectionsUnder', 'BoolBridle', 'aspect',\n",
    "       'Relative_Thickness', 'pipe_area', 'Total_Connections',\n",
    "       'area_connection', 'connection_bool', 'gas_natural',\n",
    "       'Material_Acrylonitrile-Butadiene-Styrene', 'Material_Copper',\n",
    "       'Material_Fiberglass-Reinforced Plastic', 'Material_Polyethylene',\n",
    "       'Material_Polypropylene', 'Diameter2', 'Length2', 'Pressure2',\n",
    "       'Average yearly temperature (°C)', 'Min. Temperature (°C)',\n",
    "       'Max. Temperature (°C)', 'Yearly Rainfall (mm)',\n",
    "       'Average year Humidity (%)', 'Rainy days per year (days)',\n",
    "       'Yearly Sun Hours (hours)', 'No_Inspections', 'average_severity_pipe',\n",
    "       'Average_MonthsLastRev', 'pipe_inspected_frequently',\n",
    "       'No_Previous_Incidences', 'No_Previous_Severity_High',\n",
    "       'No_Previous_Severity_Medium', 'No_Previous_Severity_Low',\n",
    "       'InspectionYear', 'MonthsLastRev', 'relative_risk',\n",
    "       'preventive_maintenance_rate', 'Probability_rate',\n",
    "       'Age_pipe_at_inspection'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show kaggle_submission\n",
    "kaggle_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ba077",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export kaggle_submission to csv\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\")\n",
    "\n",
    "kaggle_submission.to_csv('kaggle_submission_rf3_with_outliers_correlated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb47a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04183c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fddf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest classifier with regularization and gridsearch to find the best parameters for the model\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth = 2, verbose=1, n_estimators=100, max_features='auto', min_samples_leaf=1, min_samples_split=2, class_weight='balanced' )\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='coolwarm')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22deedf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfc.predict_proba(X_test)\n",
    "predictions = predictions[:, 1]\n",
    "\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use model to predict on test set\n",
    "predictions = rfc.predict_proba(X_test)\n",
    "predictions = predictions[:, 1]\n",
    "\n",
    "#show accuracy score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ed4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count null in sample_submission\n",
    "sample_submission.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886743c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a022371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7973ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop incidence in sample_submission\n",
    "sample_submission = sample_submission.drop('Incidence', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions for submission using the model\n",
    "predictions = rfc.predict_proba(sample_submission)\n",
    "predictions = predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfa498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add predictions to submission\n",
    "sample_submission ['Incidence'] = predictions\n",
    "\n",
    "#show submission\n",
    "sample_submission.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns except PipeId and Incidence in submission2 creating kaggle_submission\n",
    "kaggle_submission = sample_submission.drop(['Diameter', 'Length', 'Pressure', 'NumConnections', 'aspect',\n",
    "       'Relative_Thickness', 'area_connection', 'gas_natural',\n",
    "       'Material_Acrylonitrile-Butadiene-Styrene', 'Material_Copper',\n",
    "       'Material_Fiberglass-Reinforced Plastic', 'Material_Polyethylene',\n",
    "       'Material_Polypropylene', 'Yearly Sun Hours (hours)',\n",
    "       'Average_MonthsLastRev', 'MonthsLastRev', 'No_Inspections',\n",
    "       'No_Incidences_Total', 'Age_pipe_at_inspection',\n",
    "       'Population density (persons/sqkm)'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#show kaggle_submission\n",
    "kaggle_submission.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b85cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6372270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export kaggle_submission to csv\n",
    "os.chdir(r\"C:\\Users\\pedro\\datathon\")\n",
    "\n",
    "kaggle_submission.to_csv('kaggle_submission_new_features_baseline.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d9eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07dfa46f",
   "metadata": {},
   "source": [
    "# Lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d29fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lazypredict to  find the best model to predict on the dataset\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=1, ignore_warnings=False, custom_metric=None, predictions=True, random_state=42, n_jobs=-1)\n",
    "models, predictions = clf.fit(X_train, X_train, X_test, y_test)\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "326e4f63ad54c217260fc7be1c53acea6ef3ea6cd7ac93b3b02195c6d8fa7cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
